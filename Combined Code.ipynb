{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk tokenizer Function that returns list of bow for all movies in a given set(train/dev/test) of the given file root\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import WordPunctTokenizer        # splits all punctuations into separate tokens \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def bow_movie_nltk(root,start,end):\n",
    "    bow_per_movie = [] # list of dict where each element of bow_per_movie is bow for that movie\n",
    "    for i in range(start,end):\n",
    "        bow = defaultdict(float)\n",
    "        string = \"\"\n",
    "        for j in range(1,len(root[i])):\n",
    "            string += root[i][j].text\n",
    "\n",
    "        tokens = word_punct_tokenizer.tokenize(string)\n",
    "        l_tokens = map(lambda t: t.lower(), tokens)\n",
    "        \n",
    "        ### Lemmatizing using wordnetlemmatizer\n",
    "        l_tokens = [wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(l_tokens)]\n",
    "        ###\n",
    "        \n",
    "        for token in l_tokens:\n",
    "            bow[token] += 1.0\n",
    "        bow_per_movie.append(bow)\n",
    "    return bow_per_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk stop words, Function that returns vocab for all movies in the training set of given file root\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "def vocab_nltk(bow_per_movie):\n",
    "    vocab = defaultdict(float)\n",
    "\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    \n",
    "    #############################################\n",
    "    for bow in bow_per_movie:                   #\n",
    "        for key,value in bow.iteritems():       #    A: Code snippet to generate complete vocaublary\n",
    "            vocab[key] += value                 #\n",
    "    #############################################\n",
    "    \n",
    "    #############################################\n",
    "    for key,value in vocab.items():             #\n",
    "        if key in stop_words:                   #     B: Code snippet to remove stop words from complete vocabulary\n",
    "            vocab.pop(key)                      #\n",
    "    #############################################\n",
    "    \n",
    "    #############################################\n",
    "    for key,value in vocab.items():             #\n",
    "        if value<5:                             #     C: Code snippet to remove infrequent words from complete vocabulary\n",
    "            vocab.pop(key)\n",
    "    #############################################\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('dataset\\\\movies-data-v1.0\\\\movies-data-v1.0\\\\perscreen-7domains-train-dev.tl.xml')\n",
    "root_traindev_ps = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_per_movie_train_ps = bow_movie_nltk(root_traindev_ps,0,1147)\n",
    "bow_per_movie_test_ps = bow_movie_nltk(root_traindev_ps,1147,1464)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating and storing  feature vectors based on vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that returns feature vector for all movies in the given set(train/dev/test) of given file root\n",
    "def fvec(bow_per_movie,vocab):\n",
    "    fvec_per_movie = [] # list of lists where each element of fvec_per_movie is a feature vector for that movie\n",
    "\n",
    "    for bow in bow_per_movie:\n",
    "        fvec = []\n",
    "        for key,value in vocab.iteritems():\n",
    "            if key in bow:\n",
    "                fvec.append(bow[key])\n",
    "            else:\n",
    "                fvec.append(0)\n",
    "\n",
    "        fvec_per_movie.append(fvec)\n",
    "    return fvec_per_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that returns a list of target variables i.e. revenue for all movies in the given set(train/dev/test) of given file root\n",
    "def true_rev(start,end,root):\n",
    "    rev = []\n",
    "    for i in range(start,end):\n",
    "        rev.append(root[i][0].attrib['yvalue'])\n",
    "    rev=np.array(rev).astype(np.float)\n",
    "    return rev\n",
    "\n",
    "# Extracting revenue generated by the movies in the training set\n",
    "true_rev_train_ps = true_rev(0,1147,root_traindev_ps)\n",
    "\n",
    "# Extracting revenue generated by the movies in the test set\n",
    "true_rev_test_ps = true_rev(1147,1464,root_traindev_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocab_ps = vocab_nltk(bow_per_movie_train_ps)\n",
    "fvec_train_ps = fvec(bow_per_movie_train_ps,vocab_ps)\n",
    "fvec_test_ps = fvec(bow_per_movie_test_ps,vocab_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Training set feature vector #######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature vector for full vocabulary using nltk library\n",
    "f = open('features//fvec_train_ps_nfsi.txt', 'w')\n",
    "pickle.dump(fvec_train_ps, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Testing set feature vector #######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature vector for full vocabulary using nltk library\n",
    "f = open('features//fvec_test_ps_nfsi.txt', 'w')\n",
    "pickle.dump(fvec_test_ps, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24861\n",
      "24861\n"
     ]
    }
   ],
   "source": [
    "print len(fvec_train_ps[0])\n",
    "print len(fvec_dev_ps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "def gsv(train_x,train_y):\n",
    "    parameters = {'max_depth':[5, 100]}\n",
    "    clf = GridSearchCV(DecisionTreeRegressor(), parameters)\n",
    "    #clf = GridSearchCV(Ridge(), parameters)\n",
    "    #clf = GridSearchCV(Lasso(), parameters)\n",
    "    clf.fit(train_x, train_y)\n",
    "    return clf\n",
    "\n",
    "def train(train_x,train_y):\n",
    "    clf =  DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
    "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_cweight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "           splitter='best')\n",
    "    \n",
    "    #clf = Ridge(alpha=0.01)\n",
    "    #clf=  linear_model.Lasso(alpha=0.01)\n",
    "    clf.fit(train_x, train_y)\n",
    "    return clf\n",
    "\n",
    "def predict(clf, test_x): \n",
    "    return clf.predict(test_x)\n",
    "\n",
    "def cal_mae(y_hat,y):\n",
    "    return np.mean(abs(y_hat-y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting revenue in test set using Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "           splitter='best'),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'max_depth': [5, 100]}, pre_dispatch='2*n_jobs',\n",
      "       refit=True, return_train_score=True, scoring=None, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# Finding the best parameters for a particular Mahine Learning model using grid search\n",
    "r = gsv(fvec_train_ps,true_rev_train_ps)\n",
    "print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading feature vector for training set\n",
    "f= open('features//fvec_train_ps_nfsi.txt', 'r')\n",
    "fvec_train_ps_nfsi= pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Loading feature vector of testing set\n",
    "g= open('features//fvec_test_ps_nfsi.txt', 'r')\n",
    "fvec_dev_ps_nfsi= pickle.load(g)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_classifier_nfsi = train(fvec_train_ps_nfsi,true_rev_train_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean absolute error for dev set is  7067.13994361\n"
     ]
    }
   ],
   "source": [
    "pred_rev_test_ps = predict(trained_classifier,fvec_test_ps_nfsi) \n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting revenue in test set using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(300, input_shape=(24861,)),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(1),\n",
    "    Activation('linear'),\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(fvec_train_ps_nfsi, true_rev_train_ps, epochs=50)\n",
    "\n",
    "pred_rev_test_ps = predict(fvec_test_ps_nfsi)\n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating and storing feature vectors based on positive/negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing positive-negative words\n",
    "\n",
    "def lemmatization_for_list(l):\n",
    "    new=[]\n",
    "    for i,j in pos_tag(l):\n",
    "        if j[0].lower() in ['a','n','v']:\n",
    "            new.append(wnl.lemmatize(i,j[0].lower()))\n",
    "        else:\n",
    "            new.append(wnl.lemmatize(i))\n",
    "    return set(new)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the positive/ negative words\n",
    "pos_list=open('features//positive-words.txt','r').readlines()\n",
    "neg_list=open('features//negative-words.txt','r').readlines()\n",
    "\n",
    "# refining it\n",
    "for i in range(len(pos_list)):\n",
    "    pos_list[i]=pos_list[i].replace('\\n','')\n",
    "    \n",
    "for i in range(len(neg_list)):\n",
    "    neg_list[i]=neg_list[i].replace('\\n','')\n",
    "    \n",
    "positive=[]\n",
    "negative=[]\n",
    "\n",
    "## lemmatizing it\n",
    "positive=lemmatization_for_list(pos_list)\n",
    "negative=lemmatization_for_list(neg_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the feature vector based on the absense and presence of pos-neg words\n",
    "# For training set\n",
    "feature_train=[]\n",
    "\n",
    "for bow in bow_per_movie_train:\n",
    "    feat=[]\n",
    "    for pos in positive:\n",
    "        if pos in bow.keys():\n",
    "            feat.append(1)\n",
    "        else:\n",
    "            feat.append(0)\n",
    "    for neg in negative:\n",
    "        if neg in bow.keys():\n",
    "            feat.append(-1)\n",
    "        else:\n",
    "            feat.append(0)\n",
    "            \n",
    "    feature_train.append(feat)\n",
    "    \n",
    "# For test set\n",
    "feature_test=[]\n",
    "for bow in bow_per_movie_dev:\n",
    "    feat=[]\n",
    "    for pos in positive:\n",
    "        if pos in bow.keys():\n",
    "            feat.append(1)\n",
    "        else:\n",
    "            feat.append(0)\n",
    "    for neg in negative:\n",
    "        if neg in bow.keys():\n",
    "            feat.append(-1)\n",
    "        else:\n",
    "            feat.append(0)\n",
    "            \n",
    "    feature_test.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing training Feature vector based on positive/negative words to file\n",
    "f = open('features//train_posneg_feat.txt', 'w')\n",
    "pickle.dump(feature_train.append, f)\n",
    "f.close()\n",
    "\n",
    "# Writing testing Feature vector based on positive/negative words to file\n",
    "f = open('features//test_posneg_feat.txt', 'w')\n",
    "pickle.dump(feature_test.append, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting revenue in test set using Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_classifier_nfsi = train(feature_train,true_rev_train_ps)\n",
    "pred_rev_test_ps = predict(trained_classifier,feature_test) \n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting revenue in test set using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(300, input_shape=(5866,)),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(1),\n",
    "    Activation('linear'),\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(fvec_train_ps_nfsi, true_rev_train_ps, epochs=50)\n",
    "\n",
    "pred_rev_test_ps = predict(fvec_test_ps_nfsi)\n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(feature_train, true_rev_train_ps, epochs=50)\n",
    "\n",
    "pred_rev_test_ps = predict(feature_test)\n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating and storing feature vectors based on polarity of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns list of bow for all movies in a given set(train/dev/test) of the given file root\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def bow_movie(root,start,end):\n",
    "    review_list=[]\n",
    "    bow_per_movie = [] # list of dict where each element of bow_per_movie is bow for that movie\n",
    "    for i in range(start,end):\n",
    "        bow = defaultdict(float)\n",
    "        string = \"\"\n",
    "        for j in range(1,len(root[i])):\n",
    "            string += root[i][j].text\n",
    "\n",
    "        tokens =string.split()\n",
    "        l_tokens = map(lambda t: t.lower(), tokens)\n",
    "\n",
    "        for token in l_tokens:\n",
    "            bow[token] += 1.0\n",
    "        review_list.append(string)\n",
    "        bow_per_movie.append(bow)\n",
    "        \n",
    "    return bow_per_movie,review_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "bow_per_movie_train_to, review_list_to = bow_movie(root_traindev_to,0,1147)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "review_polarity=[0]*len(review_list_to)\n",
    "i=0\n",
    "for review in review_list_to:\n",
    "    ss = sid.polarity_scores(review)\n",
    "    review_polarity[i]=ss['compound']\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "review_polarity=np.array(review_polarity).reshape(-1,1)\n",
    "true_rev_train_ps=np.array(true_rev_train_ps).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting revenue in test set using Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_classifier_nfsi = train(review_polarity,true_rev_train_ps)\n",
    "pred_rev_test_ps = predict(trained_classifier,feature_test) \n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting revenue in test set using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(300, input_shape=(1,)),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(1),\n",
    "    Activation('linear'),\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(fvec_train_ps_nfsi, true_rev_train_ps, epochs=50)\n",
    "\n",
    "pred_rev_test_ps = predict(fvec_test_ps_nfsi)\n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(review_polarity, true_rev_train_ps, epochs=50)\n",
    "\n",
    "pred_rev_test_ps = predict(feature_test)\n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Generating and storing feature vectors based on parts of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "dict_a = {'CC':0,'CD':0,'DT':0,'EX':0,'FW':0,'IN':0,'JJ':0,'JJR':0,'JJS':0,'LS':0,'MD':0,'NN':0,'NNS':0,'NNP':0,'NNPS':0,'PDT':0,'POS':0,'PRP':0,'PRP$':0,'RB':0,'RBR':0,'RBS':0,'RP':0,'SYM':0,'TO':0,'UH':0,'VB':0,'VBD':0,'VBG':0,'VBN':0,'VBP':0,'VBZ':0,'WDT':0,'WP':0,'WP$':0,'WRB':0}\n",
    "ordered_tag = OrderedDict(sorted(dict_a.items(), key=lambda t: t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import copy\n",
    "\n",
    "def fvec_postag(bow_per_movie,ordered_tag):    # bow_per_movie is a list\n",
    "    fvec_per_movie = [] # list of lists where each element of fvec_per_movie is a feature vector for that movie\n",
    "\n",
    "    for bow in bow_per_movie:           # bow is a dict\n",
    "        tag_dict= copy.copy(ordered_tag)\n",
    "        for key,value in bow.iteritems():  \n",
    "            \n",
    "            temp = nltk.word_tokenize(key)\n",
    "            \n",
    "            try:\n",
    "                tag_dict[pos_tag(temp)[0][1]] += value\n",
    "            except:\n",
    "                pass\n",
    "        fvec_per_movie.append([j/len(bow) for j in tag_dict.values()])\n",
    "    return fvec_per_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvec_postag_train = fvec_postag(bow_per_movie_train_to,ordered_tag)\n",
    "fvec_postag_test = fvec_postag(bow_per_movie_test_to,ordered_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing feature vector to file\n",
    "f = open('features//fvec_postag_train.txt', 'w')\n",
    "pickle.dump(fvec_postag_train, f)\n",
    "f.close()\n",
    "\n",
    "f = open('features//fvec_postag_test.txt', 'w')\n",
    "pickle.dump(fvec_postag_test, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting revenue in test set using Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_classifier_nfsi = train(fvec_postag_train,true_rev_train_ps)\n",
    "pred_rev_test_ps = predict(trained_classifier,fvec_postag_test) \n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting revenue in test set using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(300, input_shape=(36,)),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(300),\n",
    "    Activation('relu'),\n",
    "    Dense(1),\n",
    "    Activation('linear'),\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(fvec_train_ps_nfsi, true_rev_train_ps, epochs=50)\n",
    "\n",
    "pred_rev_test_ps = predict(fvec_test_ps_nfsi)\n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(fvec_postag_train, true_rev_train_ps, epochs=50)\n",
    "\n",
    "pred_rev_test_ps = predict(fvec_postag_test)\n",
    "mae_rev_test_ps= cal_mae(pred_rev_dev_ps,true_rev_test_ps)\n",
    "\n",
    "print \"The mean absolute error for test set is \", mae_rev_test_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                    END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
